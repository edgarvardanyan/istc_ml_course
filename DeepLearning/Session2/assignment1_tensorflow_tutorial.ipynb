{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST master\n",
    "\n",
    "For a demo we shall solve the same digit recognition problem, but at a different scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing this homework, read some code examples written in tensorflow. There is a good repository with code examples: https://github.com/aymericdamien/TensorFlow-Examples. As we already know, we need many samples to learn :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 28, 28) (50000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.swapaxes(np.swapaxes(X_train, 1,2),2,3)\n",
    "X_val = np.swapaxes(np.swapaxes(X_val, 1,2),2,3)\n",
    "X_test = np.swapaxes(np.swapaxes(X_test, 1,2),2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining placeholders for input and target\n",
    "input_X = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], \n",
    "                         name=\"X\")\n",
    "target_y = tf.placeholder(tf.int32, shape=[None], \n",
    "                          name=\"target_Y_integer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = tf.layers.conv2d(inputs=input_X,\n",
    "                 filters=32,\n",
    "                 kernel_size=[5, 5],\n",
    "                 padding=\"same\",\n",
    "                 activation=tf.nn.relu)\n",
    "\n",
    "l2 = tf.layers.conv2d(inputs=l1,\n",
    "                 filters=32,\n",
    "                 kernel_size=[5, 5],\n",
    "                 padding=\"same\",\n",
    "                 activation=tf.nn.relu)\n",
    "\n",
    "l4 = tf.layers.max_pooling2d(inputs=l2,\n",
    "                            pool_size=[2, 2],\n",
    "                            strides=1)\n",
    "\n",
    "dropout1 = tf.layers.dropout(l4,rate=0.25)\n",
    "\n",
    "l5 = tf.layers.conv2d(inputs=dropout1,\n",
    "                        filters=64,\n",
    "                        kernel_size=[3, 3],\n",
    "                        padding=\"same\",\n",
    "                        activation=tf.nn.relu)\n",
    "\n",
    "l6 = tf.layers.conv2d(inputs=l5,\n",
    "                        filters=64,\n",
    "                        kernel_size=[3, 3],\n",
    "                        padding=\"same\",\n",
    "                        activation=tf.nn.relu)\n",
    "\n",
    "l7 = tf.layers.max_pooling2d(inputs=l6,\n",
    "                            pool_size=[2, 2],\n",
    "                            strides=2)\n",
    "\n",
    "dropout2 = tf.layers.dropout(l7,rate=0.25)\n",
    "\n",
    "l8 = tf.layers.conv2d(inputs=dropout2,\n",
    "                       filters=128,\n",
    "                       kernel_size=[2, 2],\n",
    "                       padding=\"same\",\n",
    "                       strides = (3,3),\n",
    "                       activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "input_X_reshaped = tf.layers.flatten(l8)\n",
    "\n",
    "# Fully connected layer, that takes input layer and applies 50 neurons \n",
    "# to it. Nonlinearity here is sigmoid as in logistic regression.\n",
    "# You can give a name to each layer (optional)\n",
    "dense1 = tf.layers.dense(input_X_reshaped, units=256,\n",
    "                    activation=tf.nn.sigmoid)\n",
    "dropout3 = tf.layers.dropout(dense1)\n",
    "\n",
    "# dense6 = tf.layers.dense(dropout3, units=50, \n",
    "#                      activation=tf.nn.relu)\n",
    "\n",
    "# dense7 = tf.layers.dense(dense6, units=50, \n",
    "#                      activation=tf.nn.relu)\n",
    "\n",
    "# dropout2 = tf.layers.dropout(l7)\n",
    "\n",
    "# Fully connected output layer that takes l1 as input and has \n",
    "# 10 neurons (1 for each digit).\n",
    "# This predicts scores for the classes\n",
    "dense2 = tf.layers.dense(dropout3, units=10, activation=None)\n",
    "\n",
    "# We use softmax nonlinearity to make probabilities add up to 1\n",
    "l_out = tf.nn.softmax(dense2)\n",
    "\n",
    "# Prediction\n",
    "y_predicted = tf.argmax(dense2, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution1 = tf.layers.conv2d(input_X, 32, (5,5), padding = 'same', activation = tf.nn.relu)\n",
    "convolution2 = tf.layers.conv2d(convolution1, 32, (5,5), padding = 'same', activation = tf.nn.relu)\n",
    "pooling1 = tf.layers.max_pooling2d(convolution2, (2,2), (2,2), padding = 'same')\n",
    "dropout1 = tf.layers.dropout(pooling1, 0.25)\n",
    "convolution3 = tf.layers.conv2d(dropout1, 64, (3,3), padding = 'same', activation = tf.nn.relu)\n",
    "convolution4 = tf.layers.conv2d(convolution3, 64, (3,3), padding = 'same', activation = tf.nn.relu)\n",
    "pooling2 = tf.layers.max_pooling2d(convolution4, (2,2), (2,2), padding = 'same')\n",
    "dropout2 = tf.layers.dropout(pooling2, 0.25)\n",
    "flatten = tf.layers.flatten(dropout2)\n",
    "dense1 = tf.layers.dense(flatten, 256, tf.nn.relu)\n",
    "dropout3 = tf.layers.dropout(dense1, 0.5)\n",
    "dense2 = tf.layers.dense(dropout3, 10, tf.nn.relu)\n",
    "# We use softmax nonlinearity to make probabilities add up to 1\n",
    "l_out = tf.nn.softmax(dense2)\n",
    "\n",
    "# Prediction\n",
    "y_predicted = tf.argmax(dense2, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_1/kernel:0' shape=(5, 5, 32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_3/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_4/kernel:0' shape=(2, 2, 64, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_4/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(3200, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(256, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tf.trainable_variables()\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Than you could simply\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a whole lot of work and life's short\n",
    "  * not to mention life's too short to wait for SGD to converge\n",
    "\n",
    "Instead, we shall use Tensorflow builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean categorical crossentropy as a loss function\n",
    "# - similar to logistic loss but for multiclass targets\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=target_y, logits=dense2, name=\"softmax_loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'accuracy/total:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'accuracy/count:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, update_accuracy = tf.metrics.accuracy(target_y, y_predicted)\n",
    "tf.local_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimzer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_step = optimzer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# inputs - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# outputs - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize):\n",
    "    assert len(inputs) == len(targets)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model saver.\n",
    "<br>\n",
    "See more:\n",
    "http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./checkpoints/model.ckpt\"\n",
    "saver = tf.train.Saver(max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 104.611s\n",
      "  training loss (in-iteration):\t\t0.247448\n",
      "  train accuracy:\t\t92.41 %\n",
      "  validation accuracy:\t\t98.10 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-0\n",
      "Epoch 2 of 100 took 103.059s\n",
      "  training loss (in-iteration):\t\t0.057431\n",
      "  train accuracy:\t\t98.36 %\n",
      "  validation accuracy:\t\t98.17 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-1\n",
      "Epoch 3 of 100 took 103.316s\n",
      "  training loss (in-iteration):\t\t0.039589\n",
      "  train accuracy:\t\t98.81 %\n",
      "  validation accuracy:\t\t98.82 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-2\n",
      "Epoch 4 of 100 took 102.703s\n",
      "  training loss (in-iteration):\t\t0.029860\n",
      "  train accuracy:\t\t99.13 %\n",
      "  validation accuracy:\t\t98.65 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-3\n",
      "Epoch 5 of 100 took 102.611s\n",
      "  training loss (in-iteration):\t\t0.026348\n",
      "  train accuracy:\t\t99.20 %\n",
      "  validation accuracy:\t\t98.95 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-4\n",
      "Epoch 6 of 100 took 103.761s\n",
      "  training loss (in-iteration):\t\t0.019460\n",
      "  train accuracy:\t\t99.42 %\n",
      "  validation accuracy:\t\t98.94 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-5\n",
      "Epoch 7 of 100 took 102.853s\n",
      "  training loss (in-iteration):\t\t0.017539\n",
      "  train accuracy:\t\t99.47 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-6\n",
      "Epoch 8 of 100 took 103.442s\n",
      "  training loss (in-iteration):\t\t0.013195\n",
      "  train accuracy:\t\t99.61 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-7\n",
      "Epoch 9 of 100 took 102.539s\n",
      "  training loss (in-iteration):\t\t0.013552\n",
      "  train accuracy:\t\t99.58 %\n",
      "  validation accuracy:\t\t98.89 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-8\n",
      "Epoch 10 of 100 took 103.164s\n",
      "  training loss (in-iteration):\t\t0.013245\n",
      "  train accuracy:\t\t99.60 %\n",
      "  validation accuracy:\t\t99.00 %\n",
      "  Model saved in file: ./checkpoints/model.ckpt-9\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 # amount of passes through the data\n",
    "\n",
    "batch_size = 128 # number of samples processed at each function call\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialize global wariables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "            inputs, targets = batch\n",
    "\n",
    "            _, train_err_batch, _ = sess.run(\n",
    "                [train_step, loss, update_accuracy], \n",
    "                feed_dict={input_X: inputs, target_y:targets}\n",
    "            )\n",
    "            train_err += train_err_batch\n",
    "            train_batches += 1\n",
    "        train_acc = sess.run(accuracy)\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "            inputs, targets = batch\n",
    "            sess.run(update_accuracy, feed_dict={input_X: inputs, \n",
    "                                                 target_y:targets})\n",
    "        val_acc = sess.run(accuracy)\n",
    "\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "        print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "            train_acc * 100))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc * 100))\n",
    "        \n",
    "        # save model\n",
    "        save_path = saver.save(sess, model_path, global_step=epoch)\n",
    "        print(\"  Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can restore saved parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt-99\n",
      "Model restored from file: ./checkpoints/model.ckpt-99\n",
      "Final results:\n",
      "  test accuracy:\t\t94.77 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    load_path = saver.restore(sess, saver.last_checkpoints[-1])\n",
    "    print(\"Model restored from file: %s\" % save_path)\n",
    "    \n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "        inputs, targets = batch\n",
    "        sess.run(update_accuracy, feed_dict={input_X: inputs, \n",
    "                                                   target_y:targets})\n",
    "    test_acc = sess.run(accuracy)\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc* 100))\n",
    "\n",
    "    if test_acc * 100 > 99.5:\n",
    "        print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "    else:\n",
    "        print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now improve it!\n",
    "\n",
    "* Moar layers!\n",
    "* Moar units!\n",
    "* Different nonlinearities!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
